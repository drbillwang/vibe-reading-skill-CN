"""
Gemini API å®¢æˆ·ç«¯å°è£…
æä¾›ä¸ Google Gemini API äº¤äº’çš„æ¥å£

ç‰¹æ€§ï¼š
- è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼šé‡åˆ° 429 é…é¢é™åˆ¶é”™è¯¯æ—¶ï¼Œè‡ªåŠ¨é‡è¯•æœ€å¤š 5 æ¬¡
- æ™ºèƒ½ç­‰å¾…æ—¶é—´ï¼šä»é”™è¯¯ä¿¡æ¯ä¸­æå–å»ºè®®ç­‰å¾…æ—¶é—´ï¼Œæˆ–ä½¿ç”¨é¢„å®šä¹‰åºåˆ—ï¼ˆ60/90/120/150/180 ç§’ï¼‰
- ä»£ç†æ”¯æŒï¼šæ”¯æŒé€šè¿‡ HTTP_PROXY å’Œ HTTPS_PROXY ç¯å¢ƒå˜é‡é…ç½®ä»£ç†
"""

import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import List, Dict, Optional

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class GeminiClient:
    """Gemini API å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None, proxy: Optional[str] = None):
        """
        åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯
        
        Args:
            api_key: Gemini API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ä½¿ç”¨ gemini-2.5-pro
            proxy: ä»£ç†æœåŠ¡å™¨åœ°å€ï¼ˆæ ¼å¼ï¼šhttp://host:port æˆ– https://host:portï¼‰ï¼Œç”¨äºè§£å†³åœ°ç†ä½ç½®é™åˆ¶
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY not found. Please set it in .env file or pass as parameter.")
        
        # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        self.available_models = [
            "gemini-3-pro",        # Gemini 3 æ——èˆ°æ¨¡å‹ï¼ˆæœ€æ–°ï¼Œæœ€å¼ºï¼‰
            "gemini-3",            # Gemini 3 æ ‡å‡†æ¨¡å‹
            "gemini-2.5-pro",      # Gemini 2.5 æ——èˆ°æ¨¡å‹
            "gemini-2.5-flash",    # Gemini 2.5 å¿«é€Ÿæ¨¡å‹
            "gemini-2.0-flash-exp", # å®éªŒç‰ˆæœ¬
            "gemini-1.5-pro",      # ç¨³å®šç‰ˆæœ¬
            "gemini-1.5-flash",    # å¿«é€Ÿç‰ˆæœ¬
            "gemini-pro",          # æ—§ç‰ˆæœ¬
        ]
        
        # è·å–æ¨¡å‹åç§°ï¼šå‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
        # é»˜è®¤ä½¿ç”¨ gemini-2.5-proï¼ˆç¨³å®šå¯é ï¼‰
        default_model = os.getenv("GEMINI_MODEL", "gemini-2.5-pro")
        self.model_name = model or default_model
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼ˆå…è®¸ç”¨æˆ·ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼‰
        if self.model_name not in self.available_models:
            print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ '{self.model_name}' ä¸åœ¨æ¨èåˆ—è¡¨ä¸­ï¼Œä½†å°†ç»§ç»­å°è¯•ä½¿ç”¨")
            print(f"   æ¨èæ¨¡å‹: {', '.join(self.available_models[:3])}")
        
        # é…ç½®ä»£ç†ï¼ˆå¦‚æœæä¾›ï¼‰
        proxy_url = proxy or os.getenv("HTTP_PROXY") or os.getenv("HTTPS_PROXY")
        if proxy_url:
            print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_url}")
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®© google-generativeai ä½¿ç”¨ä»£ç†
            os.environ['HTTP_PROXY'] = proxy_url
            os.environ['HTTPS_PROXY'] = proxy_url
        
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(self.model_name)
    
    def generate_content(
        self,
        prompt: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        max_output_tokens: Optional[int] = None
    ) -> str:
        """
        ç”Ÿæˆå†…å®¹ï¼ˆå¸¦æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼‰
        
        å½“é‡åˆ° API é…é¢é™åˆ¶ï¼ˆ429 é”™è¯¯ï¼‰æ—¶ï¼Œä¼šè‡ªåŠ¨é‡è¯•ï¼š
        - æœ€å¤šé‡è¯• 5 æ¬¡
        - ç­‰å¾…æ—¶é—´ï¼š60 â†’ 90 â†’ 120 â†’ 150 â†’ 180 ç§’ï¼ˆå¦‚æœé”™è¯¯ä¿¡æ¯ä¸­æ²¡æœ‰å»ºè®®æ—¶é—´ï¼‰
        - å¦‚æœé”™è¯¯ä¿¡æ¯åŒ…å«å»ºè®®ç­‰å¾…æ—¶é—´ï¼Œä¼šä¼˜å…ˆä½¿ç”¨è¯¥æ—¶é—´ + 5 ç§’ç¼“å†²
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_instruction: ç³»ç»ŸæŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_output_tokens: æœ€å¤§è¾“å‡º token æ•°
        
        Returns:
            AI ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        
        Raises:
            Exception: å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ä»å¤±è´¥ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸
        """
        generation_config = {
            "temperature": temperature,
        }
        if max_output_tokens:
            generation_config["max_output_tokens"] = max_output_tokens
        
        # å¦‚æœæœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œåˆ›å»ºæ–°çš„æ¨¡å‹å®ä¾‹
        if system_instruction:
            model = genai.GenerativeModel(
                model_name=self.model_name,
                system_instruction=system_instruction
            )
        else:
            model = self.model
        
        import time
        max_retries_per_error = 5  # æ¯æ¬¡é‡åˆ°é”™è¯¯æ—¶é‡è¯• 5 æ¬¡
        # é‡è¯•å»¶è¿Ÿåºåˆ—ï¼š60, 90, 120, 150, 180 ç§’
        retry_delays = [60, 90, 120, 150, 180]
        
        attempt = 0
        while attempt < max_retries_per_error:
            try:
                response = model.generate_content(
                    prompt,
                    generation_config=generation_config
                )
                return response.text
            except Exception as e:
                error_str = str(e)
                # æ‰“å°å®Œæ•´é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                if attempt == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡å¤±è´¥æ—¶æ‰“å°å®Œæ•´é”™è¯¯
                    print(f"  ğŸ” å®Œæ•´é”™è¯¯ä¿¡æ¯: {error_str[:500]}...")  # åªæ‰“å°å‰500å­—ç¬¦
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢é™åˆ¶é”™è¯¯ï¼ˆ429ï¼‰
                if "429" in error_str or "Resource has been exhausted" in error_str or "quota" in error_str.lower():
                    if attempt < max_retries_per_error - 1:
                        # å°è¯•ä»é”™è¯¯ä¿¡æ¯ä¸­æå–é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆå¤šç§æ ¼å¼ï¼‰
                        import re
                        retry_delay = None
                        
                        # æ ¼å¼1: "retry in 13.429413162s"
                        delay_match = re.search(r'retry in ([\d.]+)\s*s', error_str, re.IGNORECASE)
                        if delay_match:
                            retry_delay = int(float(delay_match.group(1))) + 5  # åŠ 5ç§’ç¼“å†²
                            print(f"  ğŸ“‹ ä»é”™è¯¯ä¿¡æ¯ä¸­æå–åˆ°å»ºè®®ç­‰å¾…æ—¶é—´: {delay_match.group(1)} ç§’")
                        
                        # æ ¼å¼2: "Please retry in 13.429413162s"
                        if not retry_delay:
                            delay_match = re.search(r'Please retry in ([\d.]+)\s*s', error_str, re.IGNORECASE)
                            if delay_match:
                                retry_delay = int(float(delay_match.group(1))) + 5
                                print(f"  ğŸ“‹ ä»é”™è¯¯ä¿¡æ¯ä¸­æå–åˆ°å»ºè®®ç­‰å¾…æ—¶é—´: {delay_match.group(1)} ç§’")
                        
                        # æ ¼å¼3: æ£€æŸ¥å¼‚å¸¸å¯¹è±¡çš„å±æ€§ï¼ˆGoogle API å¯èƒ½åœ¨è¿™é‡Œå­˜å‚¨ä¿¡æ¯ï¼‰
                        if not retry_delay and hasattr(e, 'retry_delay'):
                            retry_delay = int(e.retry_delay) + 5
                            print(f"  ğŸ“‹ ä»å¼‚å¸¸å¯¹è±¡ä¸­æå–åˆ°ç­‰å¾…æ—¶é—´: {e.retry_delay} ç§’")
                        
                        # å¦‚æœéƒ½æ²¡æœ‰æå–åˆ°ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„å»¶è¿Ÿåºåˆ—
                        if not retry_delay:
                            retry_delay = retry_delays[attempt]  # 60, 90, 120, 150, 180 ç§’
                            print(f"  ğŸ“‹ æœªæ‰¾åˆ°å»ºè®®ç­‰å¾…æ—¶é—´ï¼Œä½¿ç”¨é¢„å®šä¹‰å»¶è¿Ÿ: {retry_delay} ç§’")
                        
                        # ç¡®ä¿ç­‰å¾…æ—¶é—´è‡³å°‘ä¸ºåºåˆ—ä¸­çš„æœ€å°å€¼
                        retry_delay = max(retry_delay, retry_delays[0])
                        
                        print(f"  âš ï¸  API é…é¢é™åˆ¶ï¼Œç­‰å¾… {retry_delay} ç§’åé‡è¯• ({attempt + 1}/{max_retries_per_error})...")
                        time.sleep(retry_delay)
                        attempt += 1
                        continue
                    else:
                        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼ˆè®©ä¸Šå±‚å¤„ç†ï¼Œå¯èƒ½ä¼šå†æ¬¡è°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼‰
                        raise Exception(f"Error generating content: {error_str}\nå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries_per_error} æ¬¡)ï¼Œè¯·ç¨åå†è¯•æˆ–æ£€æŸ¥ API é…é¢ã€‚")
                else:
                    # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                    raise Exception(f"Error generating content: {error_str}")
    
    def generate_content_stream(
        self,
        prompt: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7
    ):
        """
        æµå¼ç”Ÿæˆå†…å®¹ï¼ˆç”¨äºé•¿æ–‡æœ¬è¾“å‡ºï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_instruction: ç³»ç»ŸæŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°
        
        Yields:
            AI ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        generation_config = {
            "temperature": temperature,
        }
        
        if system_instruction:
            model = genai.GenerativeModel(
                model_name=self.model_name,
                system_instruction=system_instruction
            )
        else:
            model = self.model
        
        try:
            response = model.generate_content(
                prompt,
                generation_config=generation_config,
                stream=True
            )
            for chunk in response:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            raise Exception(f"Error generating content: {str(e)}")
    
    def analyze_document_structure(self, document_content: str) -> Dict:
        """
        åˆ†ææ–‡æ¡£ç»“æ„ï¼ˆç« èŠ‚è¯†åˆ«ï¼‰
        
        Args:
            document_content: æ–‡æ¡£å†…å®¹
        
        Returns:
            åŒ…å«ç« èŠ‚ä¿¡æ¯çš„å­—å…¸
        """
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£çš„ç« èŠ‚ç»“æ„ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{document_content[:50000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£

è¯·è¿”å› JSON æ ¼å¼çš„ç« èŠ‚ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
- chapters: ç« èŠ‚åˆ—è¡¨ï¼Œæ¯ä¸ªç« èŠ‚åŒ…å« number, title, start_line, end_line, filename, confidence

å¦‚æœæ–‡æ¡£å¤ªé•¿ï¼Œè¯·å…ˆåˆ†æå‰ä¸€éƒ¨åˆ†ï¼Œç„¶åæˆ‘å¯ä»¥ç»§ç»­æä¾›åç»­å†…å®¹ã€‚"""
        
        response = self.generate_content(prompt, temperature=0.3)
        return response
    
    def decide_breakdown_strategy(self, chapter_content: str, chapter_info: Dict) -> Dict:
        """
        å†³å®šç« èŠ‚æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ‹†åˆ†
        
        Args:
            chapter_content: ç« èŠ‚å†…å®¹
            chapter_info: ç« èŠ‚ä¿¡æ¯
        
        Returns:
            æ‹†åˆ†å†³ç­–å’Œç­–ç•¥
        """
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ç« èŠ‚æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ‹†åˆ†ã€‚

ç« èŠ‚ä¿¡æ¯ï¼š
{chapter_info}

ç« èŠ‚å†…å®¹ï¼ˆå‰ 10000 å­—ç¬¦ï¼‰ï¼š
{chapter_content[:10000]}

è¯·è€ƒè™‘ï¼š
1. å†…å®¹çš„å¤æ‚åº¦å’Œå¯†åº¦
2. è¯­ä¹‰å®Œæ•´æ€§
3. åˆ†æè´¨é‡è¦æ±‚
4. ä¸Šä¸‹æ–‡çª—å£é™åˆ¶

è¿”å› JSON æ ¼å¼ï¼š
{{
    "needs_breakdown": true/false,
    "reason": "åŸå› ",
    "breakdown_points": [æ‹†åˆ†ç‚¹åˆ—è¡¨],
    "parts": [éƒ¨åˆ†åˆ—è¡¨]
}}"""
        
        response = self.generate_content(prompt, temperature=0.3)
        return response
    
    def analyze_chapter(
        self,
        chapter_content: str,
        previous_summary: Optional[str] = None,
        chapter_metadata: Optional[Dict] = None
    ) -> str:
        """
        åˆ†æç« èŠ‚å¹¶ç”Ÿæˆæ€»ç»“
        
        Args:
            chapter_content: ç« èŠ‚å†…å®¹
            previous_summary: å‰ä¸€ç« çš„æ€»ç»“ï¼ˆå¯é€‰ï¼‰
            chapter_metadata: ç« èŠ‚å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Markdown æ ¼å¼çš„ç« èŠ‚æ€»ç»“
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        context = ""
        if previous_summary:
            context += f"å‰ä¸€ç« æ€»ç»“ï¼š\n{previous_summary}\n\n"
        if chapter_metadata:
            context += f"ç« èŠ‚ä¿¡æ¯ï¼š{chapter_metadata}\n\n"
        
        prompt = f"""{context}

è¯·æ·±åº¦åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†çš„æ€»ç»“ã€‚

ç« èŠ‚å†…å®¹ï¼š
{chapter_content}

è¯·ç”Ÿæˆ Markdown æ ¼å¼çš„æ€»ç»“ï¼ŒåŒ…å«ï¼š
1. Executive Summaryï¼ˆç« èŠ‚æ‘˜è¦ï¼‰
2. Detailed Analysisï¼ˆè¯¦ç»†åˆ†æï¼‰
3. Key Takeawaysï¼ˆå…³é”®è¦ç‚¹ï¼‰
4. Connection to Previous Chapterï¼ˆä¸å‰ç« è”ç³»ï¼‰
5. Notable Quotesï¼ˆé‡è¦å¼•ç”¨ï¼‰

è¯·ç¡®ä¿åˆ†ææ·±å…¥ã€å‡†ç¡®ï¼Œå¹¶ä¸å‰ç« ä¿æŒè¿è´¯ã€‚"""
        
        response = self.generate_content(prompt, temperature=0.7)
        return response
